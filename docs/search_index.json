[["index.html", "Cooking with Cal-Adapt Climate Data in R Chapter 1 Introduction 1.1 General Workflow 1.2 Using Climate Data Wisely 1.3 Video Overviews", " Cooking with Cal-Adapt Climate Data in R April 2022 Chapter 1 Introduction Cal-Adapt is a website and data portal for downscaled climate data for California and neighboring states in the western USA. Developed with support and guidance from CA state agencies, it is the go-to source for modeled climate data. Coverage area for Cal-Adapt LOCA downscaled climate data caladaptR is a R package that makes it easier to import climate data from Cal-Adapt into R using the Cal-Adapt API. As shown in the graphic below, caladaptR can import Cal-Adapt data as tables or rasters. After that, you can turn to other R packages to wrangle, analyze, and visualize the results. This Cookbook provides code examples for importing data via the Cal-Adapt API, and what you can do with the data once you get it into R. In addition to this Cookbook, readers may wish to check out the following package vignettes: API Requests. Creating API requests. Large Queries. Techniques for querying large volumes of data. Rasters Part I: Download, Combine, Subset, and Compute Pixel Summaries. Basics of downloading rasters. Rasters Part II: Six-Dimensional Climate Data Cubes and Spatial Queries. Indexing and slicing rasters by date, GCM, and scenario. Rasters Part III: Downloading Rasters for Large Areas. Techniques for downloading large rasters. Collaborators Wanted! If you have R code that analyzes climate data, wed love to help you share it! Create a pull request, GitHub issue, or just email us with the code youd like to contribute. 1.1 General Workflow The general workflow for using caladaptR is: flowchart tab1 1) Determine your location(s) of interest. You can use your own points or polygons, or one of the preset areas-of-interest tab2 2) Create an API Request object tab1-&gt;tab2 tab3 3) Feed the API Request into a function that fetches data tab2-&gt;tab3 tab4 4) Wrangle the results into the format you require (e.g., filtering, sorting, joining, reshaping, add calculated columns, etc.) tab3-&gt;tab4 tab5 5) Continue on with your analysis or visualization tab4-&gt;tab5 1.2 Using Climate Data Wisely There are wise and unwise uses of climate data. If you want to get the weather forecast for San Diego on July 4, 2085, we can show you how to do that (see Chapter xx). That would of course be an unwise use of modeled climate data. To use climate data wisely and responsibly, you need to understand a little bit about how climate models work, what the forecasts do and do not represent, and follow some best practices. These topics are well beyond the scope of this Cookbook, but some general tips include: When summarizing data from climate models, always examine time spans of 2-3 decades. By definition, climate is the range of weather variability over periods of 2-3 decades or more. Thats what the climate models have been designed to characterize, and they do it very well at this time scale. Using the API, you can drill down and get the yearly, monthly, or even daily data, and derive metrics at those scales (e.g., frost days). But those metrics are only meaningful if you aggregate at least 2 or 3 decades worth. When looking at climate change, you generally should consider both the central tendency (i.e., trending up or down), as well as the range of variability. Both are important for most use cases. The biggest source of uncertainty about future climate conditions is the amount of greenhouse gas emissions. Since no knows the future, it is prudent to consider both the medium and high emissions scenarios. Cal-Adapt has both observed and modeled data for the historic period. The modeled data do a good job capturing the overall patterns of the observed record, but modeled data and observed data represent different things. They are apples and oranges, and you shouldnt compare them directly. This means if your goal is to compare the past and the future, you need to summarize 20-30 years of model historic data, and compare that to a summary of 20-30 years of modeled future data. 1.3 Video Overviews If reading is not your thing, the two videos below provide an overview of the features of caladaptR and show some sample code. "],["02_api-requests.html", "Chapter 2 API Requests 2.1 Constructing Cal-Adapt API Requests 2.2 Checking an API Request 2.3 Locations of Interest", " Chapter 2 API Requests 2.1 Constructing Cal-Adapt API Requests In order to fetch data thru the Cal-Adapt API, you have to first construct an API Request object. The basic elements of an API Request include: location(s) dataset(s) date range temporal aggregation period spatial aggregation function In practice, you build an API request object by mixing and matching different functions that provide difference pieces of the request, joining them by pipes. Example: Anatomy of an API Request Object library(caladaptr) sacramento_cap &lt;- ca_loc_pt(coords = c(-119.0, 35.4)) %&gt;% ## location ca_cvar(&quot;tasmin&quot;) %&gt;% ## climate variable (part of the dataset specification) ca_gcm(gcms[1:4]) %&gt;% ## climate models (part of the dataset specification) ca_scenario(&quot;rcp45&quot;) %&gt;% ## emissions scenario (part of the dataset specification) ca_period(&quot;year&quot;) %&gt;% ## temporal aggregation ca_years(start = 2070, end = 2099) ## date range Notes: the order of the functions that comprise an API Request object doesnt matter typically you save the API request as an object so you can feed it into other functions nearly all of the functions in caladaptr start with ca_ an API Request object doesnt contain any climate data, thats a separate step 2.2 Checking an API Request There are a few ways you can check an API Request object, prior to using it to fetch data from the server. print it to the console plot it (will show a map of the location(s)) feed it into ca_preflight() These methods will be shown below. 2.3 Locations of Interest The location(s) of interest in an API Request can be specified by: a data frame with columns for longitude-latitude coordinates a point or polygon simple feature data frame from the sf package the name of a preset area of interest An API Request object can contain multiple locations, but they must all be the same type. You cant mix and match different types of locations in the same API request. The coverage area of Cal-Adapts LOCA downscaled data is: library(sf, quietly = TRUE) loca_bnd_sf &lt;- system.file(&quot;extdata&quot;, &quot;loca_area.geojson&quot;, package = &quot;caladaptr&quot;) %&gt;% sf::st_read(quiet = TRUE) library(leaflet, quietly = TRUE) leaflet(loca_bnd_sf) %&gt;% addTiles() %&gt;% addPolygons() 2.3.1 Longitude-Latitude Coordinates You can specify the location of an API request using longitude-latitude coordinates. Use the ca_loc_pt() function: mysites_df &lt;- data.frame(id = 1:3, lon = c(-123.2, -122.4, -123.5), lat = c(42.1, 41.8, 40.5)) mysites_df mysites_cap &lt;- ca_loc_pt(coords = mysites_df[,2:3], id = mysites_df[,1]) %&gt;% ca_gcm(gcms[1:4]) %&gt;% ca_scenario(&quot;rcp45&quot;) %&gt;% ca_cvar(c(&quot;tasmax&quot;)) %&gt;% ca_period(&quot;year&quot;) %&gt;% ca_years(start = 2040, end = 2070) mysites_cap ## id lon lat ## 1 1 -123.2 42.1 ## 2 2 -122.4 41.8 ## 3 3 -123.5 40.5 ## Cal-Adapt API Request ## Location(s): ## x: -123.2, -122.4, -123.5 ## y: 42.1, 41.8, 40.5 ## Variable(s): tasmax ## Temporal aggregration period(s): year ## GCM(s): HadGEM2-ES, CNRM-CM5, CanESM2, MIROC5 ## Scenario(s): rcp45 ## Dates: 2040-01-01 to 2070-12-31 ## Notes: the data frame you pass for coords should contain two and only two columns: longitude and latitude. The column names are not important, but they must be in that order. the id argument lets you pass a vector or column of location id that will be returned with the climate data so you can join them back to the locations an API request should not contain duplicate locations 2.3.2 Simple Feature Data Frame You can query your own geometry using a simple feature data frame (sf package) with ca_loc_sf(). Points and polygons are supported. pinnacles_bnd &lt;- st_read(&quot;https://github.com/UCANR-IGIS/caladaptr-res/raw/main/geoms/pinnacles_bnd.geojson&quot;) ## Reading layer `pinnacles_bnd&#39; from data source ## `https://github.com/UCANR-IGIS/caladaptr-res/raw/main/geoms/pinnacles_bnd.geojson&#39; ## using driver `GeoJSON&#39; ## Simple feature collection with 1 feature and 5 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -121.2455 ymin: 36.4084 xmax: -121.1012 ymax: 36.56416 ## Geodetic CRS: WGS 84 pincl_cap &lt;- ca_loc_sf(loc = pinnacles_bnd) %&gt;% ca_gcm(gcms[1:4]) %&gt;% ca_scenario(&quot;rcp45&quot;) %&gt;% ca_cvar(c(&quot;tasmax&quot;)) %&gt;% ca_period(&quot;year&quot;) %&gt;% ca_years(start = 2040, end = 2070) %&gt;% ca_options(spatial_ag = &quot;mean&quot;) pincl_cap %&gt;% plot(locagrid = TRUE, static = TRUE) pincl_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found Notes the sf object needs have a CRS (which will be transformed to geographic coordinates if needed) Multipart points are not supported (use st_cast to convert them to simple points) to join the climate data back to the sf object, use the argument idfld to pass a column from the sf object with unique values 2.3.3 Preset Area of Interest The Cal-Adapt API has its own polygon areas-of-interest (also known as boundary layers). Using one of the preset areas-of-interest simplifies things, because you dont have to prepare and submit your own polygon object. The preset areas-of-interest include: aoipreset_types ## [1] &quot;censustracts&quot; &quot;counties&quot; &quot;cdistricts&quot; ## [4] &quot;ccc4aregions&quot; &quot;climregions&quot; &quot;hydrounits&quot; ## [7] &quot;irwm&quot; &quot;electricutilities&quot; &quot;wecc-load-area&quot; ## [10] &quot;evtlocations&quot; &quot;place&quot; To get data for one or more features from one of these preset AOIs, use glenn_cnty_cap &lt;- ca_loc_aoipreset(type = &quot;counties&quot;, idfld = &quot;fips&quot;, idval = &quot;06021&quot;) %&gt;% ca_cvar(&quot;pr&quot;) %&gt;% ca_scenario(&quot;rcp45&quot;) %&gt;% ca_period(&quot;day&quot;) %&gt;% ca_gcm(gcms[1:4]) %&gt;% ca_years(start = 2036, end = 2065) %&gt;% ca_options(spatial_ag = &quot;mean&quot;) glenn_cnty_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found plot(glenn_cnty_cap, locagrid = TRUE) The only hard part about using preset areas-of-interest is finding the id value(s) of the feature(s) youre interested in. For census tracts and counties, this might be a FIPS code. For watersheds, its a HUC10 code. For other preset AOIs, it may a name or something else. To find out which fields you can use to specify a feature, use the built-in constant aoipreset_idflds. For example, if we wanted to see which fields we can use to specify a congressional district, we could run: aoipreset_idflds$cdistricts ## [1] &quot;geoid&quot; &quot;id&quot; Next, to find the id value of features of interest, look at the attribute table of the layer. You can import any preset area-of-interest layer into R as a sf object using ca_aoipreset_geom(). For example, suppose we want to get historical precipitation for the north eastern most part of the Sierra Nevada range as defined by the 4th Climate Change Assessment. We can find the id for this polygon with: aoipreset_idflds$ccc4aregions ## [1] &quot;name&quot; &quot;id&quot; ccc4aregions_sf &lt;- ca_aoipreset_geom(&quot;ccc4aregions&quot;, quiet = TRUE) glimpse(ccc4aregions_sf) ## Rows: 12 ## Columns: 3 ## $ name &lt;chr&gt; &quot;Central Coast&quot;, &quot;Inland South&quot;, &quot;Los Angeles&quot;, &quot;North Coast&quot;, &quot;S~ ## $ id &lt;int&gt; 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27 ## $ geom &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((-13551745 4..., MULTIPOLYGON (((-12924787 3..., M~ ccc4aregions_sf$name ## [1] &quot;Central Coast&quot; &quot;Inland South&quot; ## [3] &quot;Los Angeles&quot; &quot;North Coast&quot; ## [5] &quot;Sacramento Valley&quot; &quot;San Diego&quot; ## [7] &quot;San Francisco Bay Area&quot; &quot;San Joaquin Valley&quot; ## [9] &quot;Sierra Nevada Mountains N Sierra&quot; &quot;Sierra Nevada Mountains NE Sierra&quot; ## [11] &quot;Sierra Nevada Mountains S Sierra&quot; &quot;Sierra Nevada Mountains SE Sierra&quot; With that info in hand, we can set up our API Object as follows: srnv_ne_cap &lt;- ca_loc_aoipreset(type = &quot;ccc4aregions&quot;, idfld = &quot;name&quot;, idval = &quot;Sierra Nevada Mountains NE Sierra&quot;) %&gt;% ca_livneh(TRUE) %&gt;% ca_cvar(c(&quot;tasmax&quot;)) %&gt;% ca_period(&quot;year&quot;) %&gt;% ca_years(start = 1980, end = 2010) %&gt;% ca_options(spatial_ag = &quot;mean&quot;) srnv_ne_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found srnv_ne_cap %&gt;% plot(locagrid = TRUE, static = FALSE) "],["03_time-series-viz.html", "Chapter 3 Time-Series Plots 3.1 Load Packages 3.2 Time Series Plots 3.3 Multiple Emissions Scenarios 3.4 Overlay a Trend Line 3.5 Historic and Future Data Together 3.6 Overlay a 32-model Ensemble 3.7 Visualize Variability with Histograms", " Chapter 3 Time-Series Plots In this chapter well look at how to make some time series plots with Cal-Adapt data. 3.1 Load Packages As always, begin by loading a bunch of packages into memory and specifying preferences for conflicting function names. library(caladaptr) library(units) library(ggplot2) library(dplyr) library(tidyr) library(lubridate) library(sf) Set conflicted preferences: library(conflicted) conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;count&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;, quiet = TRUE) 3.2 Time Series Plots In this section our goal is to create time series plot of projected maximum annual temperature for a single point, using the four recommended GCMs for California under RCP 4.5. 3.2.1 Fetch Some Data First we create an API request object that will ask the Cal-Adapt API to send back data for: maximum daily temperature, averaged by year (on the server) a point location (outside Salinas, CA) the first four CGMs (HadGEM2-ES, CNRM-CM5, CanESM2, MIROC5 - the four priority models recommended for California under the 4th Climate Change Assessment) two emissions scenarios (rcp45 and rcp85) the time period 2006-2099 salns_cap &lt;- ca_loc_pt(coords = c(-121.549945, 36.643642)) %&gt;% ca_gcm(gcms[1:4]) %&gt;% ca_scenario(c(&quot;rcp45&quot;, &quot;rcp85&quot;)) %&gt;% ca_period(&quot;year&quot;) %&gt;% ca_years(start = 2006, end = 2099) %&gt;% ca_cvar(c(&quot;tasmin&quot;, &quot;tasmax&quot;)) Check the API request for issues: salns_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found Fetch data: salns_tbl &lt;- salns_cap %&gt;% ca_getvals_tbl(quiet = TRUE) glimpse(salns_tbl) ## Rows: 1,280 ## Columns: 8 ## $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~ ## $ cvar &lt;fct&gt; tasmin, tasmin, tasmin, tasmin, tasmin, tasmin, tasmin, tasmi~ ## $ period &lt;fct&gt; year, year, year, year, year, year, year, year, year, year, y~ ## $ gcm &lt;fct&gt; HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, H~ ## $ scenario &lt;fct&gt; rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45~ ## $ spag &lt;fct&gt; none, none, none, none, none, none, none, none, none, none, n~ ## $ dt &lt;chr&gt; &quot;2020-12-31&quot;, &quot;2021-12-31&quot;, &quot;2022-12-31&quot;, &quot;2023-12-31&quot;, &quot;2024~ ## $ val [K] 281.5107 [K], 281.1210 [K], 282.2193 [K], 281.9851 [K], 281.815~ The val column contains the temperature values with the units recorded (i.e., a units object from the units package). But Kelvin is not very intuitive. Lets add a column with the temperatures in Fahrenheit. Because we already have the temperature values as a units object, we can convert them with units::set_units(). salns_degf_tbl &lt;- salns_tbl %&gt;% mutate(temp_f = units::set_units(val, degF)) glimpse(salns_degf_tbl) ## Rows: 1,280 ## Columns: 9 ## $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~ ## $ cvar &lt;fct&gt; tasmin, tasmin, tasmin, tasmin, tasmin, tasmin, tasmin, tasmi~ ## $ period &lt;fct&gt; year, year, year, year, year, year, year, year, year, year, y~ ## $ gcm &lt;fct&gt; HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, H~ ## $ scenario &lt;fct&gt; rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45~ ## $ spag &lt;fct&gt; none, none, none, none, none, none, none, none, none, none, n~ ## $ dt &lt;chr&gt; &quot;2020-12-31&quot;, &quot;2021-12-31&quot;, &quot;2022-12-31&quot;, &quot;2023-12-31&quot;, &quot;2024~ ## $ val [K] 281.5107 [K], 281.1210 [K], 282.2193 [K], 281.9851 [K], 281.815~ ## $ temp_f [degF] 47.04928 [degF], 46.34775 [degF], 48.32468 [degF], 47.90319 ~ 3.2.2 Plot 1. Single Climate Variable &amp; Single RCP Our table has a lot of data combined: two climate variables, four GCMs, and two emissions scenarios. Each combo of those variables constitutes a time series, so we have 16 time series in total. Well start by plotting just a single time series (one climate variable, one GCM, one scenario). Because our table is in long format, we can easily filter it with dplyr::filter(): single_series_tbl &lt;- salns_degf_tbl %&gt;% filter(cvar == &quot;tasmax&quot;, gcm == &quot;HadGEM2-ES&quot;, scenario == &quot;rcp45&quot;) %&gt;% select(dt, cvar, gcm, scenario, period, temp_f) glimpse(single_series_tbl) ## Rows: 80 ## Columns: 6 ## $ dt &lt;chr&gt; &quot;2020-12-31&quot;, &quot;2021-12-31&quot;, &quot;2022-12-31&quot;, &quot;2023-12-31&quot;, &quot;2024~ ## $ cvar &lt;fct&gt; tasmax, tasmax, tasmax, tasmax, tasmax, tasmax, tasmax, tasma~ ## $ gcm &lt;fct&gt; HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, HadGEM2-ES, H~ ## $ scenario &lt;fct&gt; rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45, rcp45~ ## $ period &lt;fct&gt; year, year, year, year, year, year, year, year, year, year, y~ ## $ temp_f [degF] 67.86865 [degF], 69.67442 [degF], 69.38894 [degF], 68.93449 ~ Plot with ggplot: ggplot(data = single_series_tbl, aes(x = as.Date(dt), y = as.numeric(temp_f))) + geom_line() + labs(title = &quot;Predicted Maximum Daily Temperature Averaged by Year&quot;, caption = &quot;Location: Salinas, CA. GCM: HadGEM2-ES; Emissions scenario: RCP4.5&quot;, x = &quot;year&quot;, y = &quot;temp (F)&quot;) If you want to plot multiple GCMs, you can differentiate them with colors. ggplot makes this easy, and will even create a legend for you. But although we can easily add lots of lines to a plot, it makes to sense to limit each plot to a single climate variables and a single emissions scenario. mult_gcm_tbl &lt;- salns_degf_tbl %&gt;% filter(cvar == &quot;tasmax&quot;, scenario == &quot;rcp45&quot;) %&gt;% select(dt, gcm, temp_f) ggplot(data = mult_gcm_tbl, aes(x = as.Date(dt), y = as.numeric(temp_f))) + geom_line(aes(color=gcm)) + labs(title = &quot;Predicted Maximum Daily Temperature Averaged by Year&quot;, caption = &quot;Location: Salinas, CA. Emissions scenario: RCP4.5&quot;, x = &quot;year&quot;, y = &quot;temp (F)&quot;) 3.3 Multiple Emissions Scenarios To plot multiple emissions scenarios, you probably want to use facets: mult_scen_tbl &lt;- salns_degf_tbl %&gt;% filter(cvar == &quot;tasmax&quot;) %&gt;% select(dt, gcm, scenario, temp_f) ggplot(data = mult_scen_tbl, aes(x = as.Date(dt), y = as.numeric(temp_f))) + geom_line(aes(color=gcm)) + facet_grid(scenario ~ .) + labs(title = &quot;Predicted Maximum Daily Temperature Averaged by Year&quot;, caption = &quot;Location: Salinas, CA&quot;, x = &quot;year&quot;, y = &quot;temp (F)&quot;) 3.4 Overlay a Trend Line ggplot can add a trend line for you (by adding geom_smooth), but you have to decide whether you want a trend line for each individual GCM, or perhaps all GCMs in the same emissions scenario. Below we overlay a trend line for each emissions scenario. ggplot(data = mult_scen_tbl, aes(x = as.Date(dt), y = as.numeric(temp_f))) + geom_line(aes(color=gcm)) + geom_smooth(formula = y ~ x, method=lm, se=FALSE, col=&#39;red&#39;, size=1) + facet_grid(scenario ~ .) + labs(title = &quot;Predicted Maximum Daily Temperature Averaged by Year&quot;, caption = &quot;Location: Salinas, CA&quot;, x = &quot;year&quot;, y = &quot;temp (F)&quot;) 3.5 Historic and Future Data Together Many studies on the impact of climate change look at the past as a reference for future climate conditions. Here well create a time series plot that goes all the way back to 1950. The first question you have to ask is what data should I use for the historic period?. Your first reaction might be to use observed historic data (i.e., from weather stations). You can certainly do that, as the Cal-Adapt API has both the Livneh and gridMet datasets (both of which are observed data interpolated from weather stations). However we dont have observed data for the future. This presents a problem because comparing observed past data with modeled future data is like comparing apples and oranges. They represent different things. A better option is to compare the modeled future climate with a modeled historic climate. This may seem counter-intuitive, but the climate models have been trained from historic data, and have been shown to do a very good job at capturing the climate envelope of the past. First we need to get the historic modeled data, for the same climate variable, GCMs, and location. caladaptR has no problem combining multiple datasets in the same API call, however in this case we need to make a separate API call for the historic data because the dates are different. salns_hist_cap &lt;- ca_loc_pt(coords = c(-121.549945, 36.643642)) %&gt;% ca_gcm(gcms[1:4]) %&gt;% ca_scenario(&quot;historical&quot;) %&gt;% ca_period(&quot;year&quot;) %&gt;% ca_years(start = 1950, end = 2005) %&gt;% ca_cvar(c(&quot;tasmin&quot;, &quot;tasmax&quot;)) salns_hist_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found Next we get the values. While were at it, we can add the column for degrees Fahrenheit: salns_hist_tbl &lt;- salns_hist_cap %&gt;% ca_getvals_tbl(quiet = TRUE) %&gt;% mutate(temp_f = units::set_units(val, degF)) head(salns_hist_tbl) ## # A tibble: 6 x 9 ## id cvar period gcm scenario spag dt val temp_f ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; [K] [degF] ## 1 1 tasmin year HadGEM2-ES historical none 1950-12-31 280. 45.0 ## 2 1 tasmin year HadGEM2-ES historical none 1951-12-31 280. 44.9 ## 3 1 tasmin year HadGEM2-ES historical none 1952-12-31 282. 47.6 ## 4 1 tasmin year HadGEM2-ES historical none 1953-12-31 282. 48.1 ## 5 1 tasmin year HadGEM2-ES historical none 1954-12-31 281. 46.9 ## 6 1 tasmin year HadGEM2-ES historical none 1955-12-31 281. 46.0 Our tibbles for the past and future time periods have identical columns, so we can stack them. While were at it, we can convert the dt column from character to Date: salns_all_tbl &lt;- salns_degf_tbl %&gt;% bind_rows(salns_hist_tbl) %&gt;% mutate(dt = as.Date(dt)) head(salns_all_tbl) ## # A tibble: 6 x 9 ## id cvar period gcm scenario spag dt val temp_f ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;date&gt; [K] [degF] ## 1 1 tasmin year HadGEM2-ES rcp45 none 2020-12-31 282. 47.0 ## 2 1 tasmin year HadGEM2-ES rcp45 none 2021-12-31 281. 46.3 ## 3 1 tasmin year HadGEM2-ES rcp45 none 2022-12-31 282. 48.3 ## 4 1 tasmin year HadGEM2-ES rcp45 none 2023-12-31 282. 47.9 ## 5 1 tasmin year HadGEM2-ES rcp45 none 2024-12-31 282. 47.6 ## 6 1 tasmin year HadGEM2-ES rcp45 none 2025-12-31 282. 47.5 summary(salns_all_tbl %&gt;% select(cvar, gcm, scenario, dt, temp_f)) ## cvar gcm scenario dt ## tasmax:864 CanESM2 :432 rcp45 :640 Min. :1950-12-31 ## tasmin:864 CNRM-CM5 :432 rcp85 :640 1st Qu.:2004-09-30 ## HadGEM2-ES:432 historical:448 Median :2046-07-01 ## MIROC5 :432 Mean :2039-03-29 ## 3rd Qu.:2073-04-01 ## Max. :2099-12-31 ## temp_f ## Min. :43.99 ## 1st Qu.:48.73 ## Median :60.94 ## Mean :59.49 ## 3rd Qu.:69.99 ## Max. :78.35 To plot the time series from 1950 thru 2099, we have to pick one of the future scenarios. salns_histrcp45_tbl &lt;- salns_all_tbl %&gt;% filter(cvar == &quot;tasmax&quot;, scenario %in% c(&quot;historical&quot;, &quot;rcp45&quot;)) %&gt;% select(dt, gcm, temp_f) salns_histrcp45_tbl ## # A tibble: 544 x 3 ## dt gcm temp_f ## &lt;date&gt; &lt;fct&gt; [degF] ## 1 2020-12-31 HadGEM2-ES 67.9 ## 2 2021-12-31 HadGEM2-ES 69.7 ## 3 2022-12-31 HadGEM2-ES 69.4 ## 4 2023-12-31 HadGEM2-ES 68.9 ## 5 2024-12-31 HadGEM2-ES 68.7 ## 6 2025-12-31 HadGEM2-ES 69.6 ## 7 2026-12-31 HadGEM2-ES 68.9 ## 8 2027-12-31 HadGEM2-ES 70.1 ## 9 2028-12-31 HadGEM2-ES 67.3 ## 10 2029-12-31 HadGEM2-ES 67.6 ## # ... with 534 more rows ggplot(data = salns_histrcp45_tbl, aes(x = as.Date(dt), y = as.numeric(temp_f))) + geom_line(aes(color=gcm)) + labs(title = &quot;Predicted Maximum Daily Temperature Averaged by Year&quot;, caption = &quot;Location: Salinas, CA. Emissions scenarios: Historical (1950-2005), RCP4.5 (2006-2099)&quot;, x = &quot;year&quot;, y = &quot;temp (F)&quot;) 3.6 Overlay a 32-model Ensemble In the examples above, we plotted the 4 priority GCMs out of the 10 recommended GCMs for California. But there are 22 other GCMs out there. To compare how the four (or ten) GCMs in a plot compare to the range of variability of all 32 GCMs, we can overlay the 32-ensemble model. The 32-ensemble model is available for many but not all of the climate variables, temporal aggregations, or emission scenarios. You can see which ensemble raster series are available by searching the data catalog for ens32: ensem_rasterseries_df &lt;- ca_catalog_search(&quot;ens32&quot;, quiet = TRUE) nrow(ensem_rasterseries_df) ## [1] 207 ensem_rasterseries_df[1:6, c(&quot;slug&quot;, &quot;name&quot;)] ## slug ## 1 cdd_year_ens32avg_historical ## 2 cdd_year_ens32avg_rcp45 ## 3 cdd_year_ens32avg_rcp85 ## 4 cddm_30y_ens32max_historical ## 5 cddm_30y_ens32max_rcp45 ## 6 cddm_30y_ens32max_rcp85 ## name ## 1 Average number of cooling degree days per year from 32 models historical ## 2 Average number of cooling degree days per year from 32 models rcp45 ## 3 Average number of cooling degree days per year from 32 models rcp85 ## 4 30 year annual average maximum length of dry spell from 32 model maximum historical ## 5 30 year annual average maximum length of dry spell from 32 model maximum rcp45 ## 6 30 year annual average maximum length of dry spell from 32 model maximum rcp85 There are 207 raster series that are derived from the 32-ensemble models. caladaptr doesnt have convenience functions that you can string together to specify the 32-ensemble raster series, so you need to find the slug(s) by searching the data catalog (see above). To overlay the 32-ensemble of daily maximum averaged by year, we need to first download the following slugs: salns_32ens_cap &lt;- ca_loc_pt(coords = c(-121.549945, 36.643642)) %&gt;% ca_slug(c(&quot;tasmax_year_ens32min_rcp45&quot;, &quot;tasmax_year_ens32max_rcp45&quot;, &quot;tasmin_year_ens32min_rcp45&quot;, &quot;tasmin_year_ens32max_rcp45&quot;)) %&gt;% ca_years(start = 2006, end = 2099) salns_32ens_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found salns_32ens_tbl &lt;- salns_32ens_cap %&gt;% ca_getvals_tbl(quiet = TRUE) glimpse(salns_32ens_tbl) ## Rows: 376 ## Columns: 5 ## $ id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ ## $ slug &lt;chr&gt; &quot;tasmax_year_ens32min_rcp45&quot;, &quot;tasmax_year_ens32min_rcp45&quot;, &quot;tasm~ ## $ spag &lt;fct&gt; none, none, none, none, none, none, none, none, none, none, none,~ ## $ dt &lt;chr&gt; &quot;2006-12-31&quot;, &quot;2007-12-31&quot;, &quot;2008-12-31&quot;, &quot;2009-12-31&quot;, &quot;2010-12-~ ## $ val [K] 291.6819 [K], 292.1163 [K], 292.0294 [K], 292.1826 [K], 291.7250 [K~ We need to make two changes: convert the date column from character to date, and add a column for degrees F. salns_32ens_tbl &lt;- salns_32ens_tbl %&gt;% mutate(dt = as.Date(dt), temp_f = units::set_units(val, degF)) head(salns_32ens_tbl) ## # A tibble: 6 x 6 ## id slug spag dt val temp_f ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;date&gt; [K] [degF] ## 1 1 tasmax_year_ens32min_rcp45 none 2006-12-31 292. 65.4 ## 2 1 tasmax_year_ens32min_rcp45 none 2007-12-31 292. 66.1 ## 3 1 tasmax_year_ens32min_rcp45 none 2008-12-31 292. 66.0 ## 4 1 tasmax_year_ens32min_rcp45 none 2009-12-31 292. 66.3 ## 5 1 tasmax_year_ens32min_rcp45 none 2010-12-31 292. 65.4 ## 6 1 tasmax_year_ens32min_rcp45 none 2011-12-31 292. 66.3 Now we can plot the 32-ensemble min and max of the daily maximum temperature averaged by year as individual lines: ens_minmax_tbl &lt;- salns_32ens_tbl %&gt;% filter(slug %in% c(&quot;tasmax_year_ens32min_rcp45&quot;, &quot;tasmax_year_ens32max_rcp45&quot;)) %&gt;% select(dt, slug, temp_f) ggplot(data = ens_minmax_tbl, aes(x = dt, y = as.numeric(temp_f))) + geom_line(aes(color=slug)) + labs(x = &quot;year&quot;, y = &quot;temp (F)&quot;) To have the area between the lines show up as a shaded area, were going to use geom_ribbon. But geom_ribbon requires that the min and max series to be in separate columns (i.e., wide format instead of long format). Hence before we can plot it we need to reshape the data: ens_minmax_wide_tbl &lt;- ens_minmax_tbl %&gt;% tidyr::pivot_wider(names_from = slug, values_from = temp_f, id_cols = dt) head(ens_minmax_wide_tbl) ## # A tibble: 6 x 3 ## dt tasmax_year_ens32min_rcp45 tasmax_year_ens32max_rcp45 ## &lt;date&gt; [degF] [degF] ## 1 2006-12-31 65.4 70.2 ## 2 2007-12-31 66.1 70.2 ## 3 2008-12-31 66.0 69.7 ## 4 2009-12-31 66.3 70.8 ## 5 2010-12-31 65.4 69.8 ## 6 2011-12-31 66.3 71.0 Now we can plot the area between the lines: ggplot(data = ens_minmax_wide_tbl, aes(x = dt)) + geom_ribbon(aes(ymin = as.numeric(tasmax_year_ens32min_rcp45), ymax = as.numeric(tasmax_year_ens32max_rcp45)), fill = &quot;gray70&quot;) ## This works also (don&#39;t convert the units) ggplot(data = ens_minmax_wide_tbl, aes(x = dt)) + geom_ribbon(aes(ymin = tasmax_year_ens32min_rcp45, ymax = tasmax_year_ens32max_rcp45), fill = &quot;gray80&quot;) Now we can combine our two plots. Well start with the geom_ribbon and then lay the data series on top. ggplot(data = ens_minmax_wide_tbl, aes(x = dt)) + geom_ribbon(aes(ymin = as.numeric(tasmax_year_ens32min_rcp45), ymax = as.numeric(tasmax_year_ens32max_rcp45)), fill = &quot;gray80&quot;) + geom_line(data = mult_gcm_tbl, mapping = aes(x = as.Date(dt), y = as.numeric(temp_f), color = gcm)) + labs(title = &quot;Predicted Maximum Daily Temperature Averaged by Year&quot;, caption = &quot;Location: Salinas, CA. Emissions scenario: RCP4.5. Shaded background = 32-GCM ensemble&quot;, x = &quot;year&quot;, y = &quot;temp (F)&quot;) 3.7 Visualize Variability with Histograms Some climate variables like precipitation dont have a strong increasing or decreasing trend, but may see higher variability. Histograms are a good way to visualize distribution. Coming soon "],["04_metrics-methods.html", "Chapter 4 Methods for Computing Climate Metrics 4.1 Load packages 4.2 Fetch Some Sample Data 4.3 Time Slicing 4.4 Daily Climate Metrics 4.5 Diurnal Temperature Range 4.6 Daily Threshhold Events 4.7 Counting Consecutive Events", " Chapter 4 Methods for Computing Climate Metrics In this Chapter, well explore some data wrangling techniques that are commonly used in computing climate metrics. 4.1 Load packages As usual, start by loading a bunch of packages into memory and specifying our package preferences for conflicting function names: library(caladaptr) library(units) library(ggplot2) library(dplyr) library(tidyr) library(lubridate) library(sf) Set conflicted preferences: library(conflicted) conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;count&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;, quiet = TRUE) 4.2 Fetch Some Sample Data For the examples in this chapter, well work with late-century daily temperature data for a location near Bakersfield, CA in the southern San Joaquin Valley. bkrfld_cap &lt;- ca_loc_pt(coords = c(-119.151062, 35.261321)) %&gt;% ca_gcm(gcms[1:4]) %&gt;% ca_scenario(c(&quot;rcp45&quot;, &quot;rcp85&quot;)) %&gt;% ca_period(&quot;day&quot;) %&gt;% ca_years(start = 2070, end = 2099) %&gt;% ca_cvar(c(&quot;tasmin&quot;, &quot;tasmax&quot;)) bkrfld_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found plot(bkrfld_cap) Fetch data: bkrfld_tbl &lt;- bkrfld_cap %&gt;% ca_getvals_tbl(quiet = TRUE) %&gt;% mutate(dt = as.Date(dt), temp_f = units::set_units(val, degF)) head(bkrfld_tbl) ## # A tibble: 6 x 9 ## id cvar period gcm scenario spag dt val temp_f ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;date&gt; [K] [degF] ## 1 1 tasmin day HadGEM2-ES rcp45 none 2070-01-01 278. 41.1 ## 2 1 tasmin day HadGEM2-ES rcp45 none 2070-01-02 278. 40.6 ## 3 1 tasmin day HadGEM2-ES rcp45 none 2070-01-03 284. 51.7 ## 4 1 tasmin day HadGEM2-ES rcp45 none 2070-01-04 276. 37.6 ## 5 1 tasmin day HadGEM2-ES rcp45 none 2070-01-05 280. 45.0 ## 6 1 tasmin day HadGEM2-ES rcp45 none 2070-01-06 280. 44.5 4.3 Time Slicing Often an analysis requires you to slice climate data into specific periods that are meaningful for a specific study. For example, water years start on October 1 and run through the following season. Or you might just be interested in the winter months, when tree crops or bugs are particularly sensitive to temperatures. In this section, well look at different techniques for time-slicing climate data. The general approach is to add a column in the table that identifies the time slice. (If youre working with rasters, the idea is similar but you add an attribute value to the layer). Once you have the time-slice identifiers in your table, you can easily filter or group on that column to compute summaries for each time-slice. lubridate is your ally when it comes to extracting date parts. For example to add columns for date parts like year, month, week, and ordinal date, we can use the standard mutate() with date part functions from lubridate: bkrfld_dtprts_tbl &lt;- bkrfld_tbl %&gt;% mutate(year = lubridate::year(dt), month = lubridate::month(dt), week = lubridate::week(dt), yday = lubridate::yday(dt)) %&gt;% select(dt, year, month, week, yday, cvar, gcm, scenario, temp_f) bkrfld_dtprts_tbl %&gt;% slice(1:20) ## # A tibble: 20 x 9 ## dt year month week yday cvar gcm scenario temp_f ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; [degF] ## 1 2070-01-01 2070 1 1 1 tasmin HadGEM2-ES rcp45 41.1 ## 2 2070-01-02 2070 1 1 2 tasmin HadGEM2-ES rcp45 40.6 ## 3 2070-01-03 2070 1 1 3 tasmin HadGEM2-ES rcp45 51.7 ## 4 2070-01-04 2070 1 1 4 tasmin HadGEM2-ES rcp45 37.6 ## 5 2070-01-05 2070 1 1 5 tasmin HadGEM2-ES rcp45 45.0 ## 6 2070-01-06 2070 1 1 6 tasmin HadGEM2-ES rcp45 44.5 ## 7 2070-01-07 2070 1 1 7 tasmin HadGEM2-ES rcp45 40.9 ## 8 2070-01-08 2070 1 2 8 tasmin HadGEM2-ES rcp45 47.7 ## 9 2070-01-09 2070 1 2 9 tasmin HadGEM2-ES rcp45 50.5 ## 10 2070-01-10 2070 1 2 10 tasmin HadGEM2-ES rcp45 46.7 ## 11 2070-01-11 2070 1 2 11 tasmin HadGEM2-ES rcp45 39.8 ## 12 2070-01-12 2070 1 2 12 tasmin HadGEM2-ES rcp45 42.9 ## 13 2070-01-13 2070 1 2 13 tasmin HadGEM2-ES rcp45 35.5 ## 14 2070-01-14 2070 1 2 14 tasmin HadGEM2-ES rcp45 34.1 ## 15 2070-01-15 2070 1 3 15 tasmin HadGEM2-ES rcp45 39.4 ## 16 2070-01-16 2070 1 3 16 tasmin HadGEM2-ES rcp45 43.3 ## 17 2070-01-17 2070 1 3 17 tasmin HadGEM2-ES rcp45 44.3 ## 18 2070-01-18 2070 1 3 18 tasmin HadGEM2-ES rcp45 45.5 ## 19 2070-01-19 2070 1 3 19 tasmin HadGEM2-ES rcp45 49.5 ## 20 2070-01-20 2070 1 3 20 tasmin HadGEM2-ES rcp45 44.8 To plot the distribution of winter time daily minimum temperatures (i.e., to identify frost days), we could use the month column to get only dates in December, January, and February: bkrfld_wintrmth_lows_tbl &lt;- bkrfld_dtprts_tbl %&gt;% filter(cvar == &quot;tasmin&quot;, month %in% c(12, 1, 2)) table(bkrfld_wintrmth_lows_tbl$month) ## ## 1 2 12 ## 7440 6776 7440 Plot histogram: ggplot(bkrfld_wintrmth_lows_tbl, aes(x=temp_f)) + geom_histogram() + facet_grid(scenario ~ .) + labs(title = &quot;Distribution of Winter Nightime Lows, 2070-2099&quot;, subtitle = &quot;Bakersfield, CA&quot;, caption = paste0(&quot;GCMs: &quot;, paste(gcms[1:4], collapse = &quot;, &quot;), &quot;. Months: Dec, Jan, and Feb.&quot;), x = &quot;night time low (F)&quot;, y = &quot;count&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we want use the standard definition of the winter season, we could alternately filter winter days by their ordinal date number (aka Julian date). Winter starts on December 21 (day 355 of non-leap years) and ends on March 20 (day 79). bkrfld_wintrssn_tbl &lt;- bkrfld_dtprts_tbl %&gt;% filter(cvar == &quot;tasmin&quot;, yday &gt;= 355 | yday &lt;= 79) ggplot(bkrfld_wintrssn_tbl, aes(x=temp_f)) + geom_histogram() + facet_grid(scenario ~ .) + labs(title = &quot;Distribution of Winter Nightime Lows, 2070-2099&quot;, subtitle = &quot;Bakersfield, CA&quot;, caption = paste0(&quot;GCMs: &quot;, paste(gcms[1:4], collapse = &quot;, &quot;), &quot;. December 21 - March 20.&quot;), x = &quot;night time low (F)&quot;, y = &quot;count&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Time slices can also be used for grouping. The following expression computes the average nightly low for the summer months for each RCP (all GCMs and years combined): bkrfld_dtprts_tbl %&gt;% filter(month %in% 6:8, cvar == &quot;tasmin&quot;) %&gt;% group_by(month, scenario) %&gt;% summarise(avg_temp = mean(temp_f), .groups = &quot;drop&quot;) %&gt;% mutate(month = month.name[month]) %&gt;% tidyr::pivot_wider(id_cols = month, names_from = scenario, values_from = avg_temp) ## # A tibble: 3 x 3 ## month rcp45 rcp85 ## &lt;chr&gt; [degF] [degF] ## 1 June 68.1 71.5 ## 2 July 74.1 78.0 ## 3 August 73.8 77.8 Sometimes the time period of interest spans two calendar years. A water year for example starts on October 1 and goes thru the end of September the following year. Some agricultural periods (such as winter dormancy) may also start in the fall and continue into the new year. Slicing your data by a time period that spans calendar years is done in the same manner - you add a column to the table for period identifier. Below we add a column for water year (which conventionally are designated by the calendar year in which it ends): bkrfld_wtryr_tbl &lt;- bkrfld_tbl %&gt;% mutate(water_yr = year(dt) + if_else(month(dt) &gt;= 10, 1, 0)) %&gt;% select(dt, water_yr, cvar, gcm, scenario, temp_f) bkrfld_wtryr_tbl %&gt;% sample_n(10) ## # A tibble: 10 x 6 ## dt water_yr cvar gcm scenario temp_f ## &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; [degF] ## 1 2089-12-08 2090 tasmax CNRM-CM5 rcp45 63.6 ## 2 2086-10-29 2087 tasmin MIROC5 rcp85 55.7 ## 3 2076-10-05 2077 tasmin CNRM-CM5 rcp85 52.3 ## 4 2085-08-15 2085 tasmin HadGEM2-ES rcp45 69.1 ## 5 2074-10-15 2075 tasmax CNRM-CM5 rcp45 95.7 ## 6 2097-07-18 2097 tasmax CNRM-CM5 rcp45 94.6 ## 7 2073-03-08 2073 tasmin MIROC5 rcp45 46.2 ## 8 2092-05-29 2092 tasmin MIROC5 rcp85 69.4 ## 9 2095-07-28 2095 tasmax MIROC5 rcp45 96.0 ## 10 2093-10-04 2094 tasmin MIROC5 rcp85 58.8 4.4 Daily Climate Metrics Many climate analyses require metrics computed on a daily time scale. For example, to see how frost exposure might change over time, we may have to look at the lowest daily temperature. This presents a small conundrum, because climate models are not weather forecasts, and best practices tell us that we dont look at less than 30 years. But it would be silly to average the daily low temperatures by month or year and use that as a proxy for frost exposure. The general approach in these cases is compute the daily metrics as if you were dealing with observed data, but then to aggregate the metrics over bigger periods of time and space. For example, you could classify each individual day in frost / non-frost, and then count the number of predicted frost days over a 30-year interval. 4.5 Diurnal Temperature Range Diurnal Temperature Range (DTR) is the difference between daily min and max temperature (Parker et al. 2022). The magnitude of DTR can impact wine grape development and taste. In the example below, we calculate DTR from November thru February. The first step is to separate the minimum and maximum daily temperatures into separate columns: bkrfld_wide_tbl &lt;- bkrfld_tbl %&gt;% filter(month(dt) %in% c(11,12,1,2)) %&gt;% tidyr::pivot_wider(id_cols = c(dt, gcm, scenario), names_from = cvar, values_from = temp_f) head(bkrfld_wide_tbl) ## # A tibble: 6 x 5 ## dt gcm scenario tasmin tasmax ## &lt;date&gt; &lt;fct&gt; &lt;fct&gt; [degF] [degF] ## 1 2070-01-01 HadGEM2-ES rcp45 41.1 67.2 ## 2 2070-01-02 HadGEM2-ES rcp45 40.6 63.2 ## 3 2070-01-03 HadGEM2-ES rcp45 51.7 67.0 ## 4 2070-01-04 HadGEM2-ES rcp45 37.6 53.9 ## 5 2070-01-05 HadGEM2-ES rcp45 45.0 54.8 ## 6 2070-01-06 HadGEM2-ES rcp45 44.5 57.9 Now we can compute DTR: bkrfld_dtr_tbl &lt;- bkrfld_wide_tbl %&gt;% mutate(dtr = tasmax - tasmin) bkrfld_dtr_tbl %&gt;% head() ## # A tibble: 6 x 6 ## dt gcm scenario tasmin tasmax dtr ## &lt;date&gt; &lt;fct&gt; &lt;fct&gt; [degF] [degF] [degF] ## 1 2070-01-01 HadGEM2-ES rcp45 41.1 67.2 26.1 ## 2 2070-01-02 HadGEM2-ES rcp45 40.6 63.2 22.7 ## 3 2070-01-03 HadGEM2-ES rcp45 51.7 67.0 15.3 ## 4 2070-01-04 HadGEM2-ES rcp45 37.6 53.9 16.3 ## 5 2070-01-05 HadGEM2-ES rcp45 45.0 54.8 9.85 ## 6 2070-01-06 HadGEM2-ES rcp45 44.5 57.9 13.4 We can show the results with a box plot: ggplot(bkrfld_dtr_tbl, aes(x=scenario, y = dtr)) + geom_boxplot() + labs(title = &quot;Diurnal Temperature Range, 2070-2099&quot;, subtitle = &quot;Bakersfield, CA&quot;, caption = paste0(&quot;GCMs: &quot;, paste(gcms[1:4], collapse = &quot;, &quot;), &quot;. Temporal period: Nov-Feb.&quot;), x = &quot;Emission scenarios&quot;, y = &quot;diurnal temperature range&quot;) 4.6 Daily Threshhold Events Many climate analyses involve a threshold event, such as temperature above or below a certain value. These tend to be easy to compute using an expression that returns TRUE or FALSE. Subsequently, you can count the number of threshold events using sum() (when you sum logical values TRUEs become 1 and FALSE becomes 0). Below we compute the number of Hot Days per year, where a Hot Day is defined as the maximum temperature over 38 °C (100.4 °F) (Parker et al. 2022). We need to keep gcm and scenario as well be grouping on those columns next. bkrfld_hotday_tbl &lt;- bkrfld_tbl %&gt;% filter(cvar == &quot;tasmax&quot;) %&gt;% mutate(hotday_yn = temp_f &gt;= units::set_units(38, degC), year = year(dt)) %&gt;% select(dt, year, cvar, scenario, gcm, temp_f, hotday_yn) bkrfld_hotday_tbl %&gt;% head() ## # A tibble: 6 x 7 ## dt year cvar scenario gcm temp_f hotday_yn ## &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; [degF] &lt;lgl&gt; ## 1 2070-01-01 2070 tasmax rcp45 HadGEM2-ES 67.2 FALSE ## 2 2070-01-02 2070 tasmax rcp45 HadGEM2-ES 63.2 FALSE ## 3 2070-01-03 2070 tasmax rcp45 HadGEM2-ES 67.0 FALSE ## 4 2070-01-04 2070 tasmax rcp45 HadGEM2-ES 53.9 FALSE ## 5 2070-01-05 2070 tasmax rcp45 HadGEM2-ES 54.8 FALSE ## 6 2070-01-06 2070 tasmax rcp45 HadGEM2-ES 57.9 FALSE Now we can group by year and scenario to compare how the average number of hot days per year looks for each RCP. bkrfld_numhd_tbl &lt;- bkrfld_hotday_tbl %&gt;% group_by(year, scenario, gcm) %&gt;% summarise(num_hotday = sum(hotday_yn)) ## `summarise()` has grouped output by &#39;year&#39;, &#39;scenario&#39;. You can override using ## the `.groups` argument. bkrfld_numhd_tbl %&gt;% head() ## # A tibble: 6 x 4 ## # Groups: year, scenario [2] ## year scenario gcm num_hotday ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2070 rcp45 CanESM2 75 ## 2 2070 rcp45 CNRM-CM5 58 ## 3 2070 rcp45 HadGEM2-ES 91 ## 4 2070 rcp45 MIROC5 70 ## 5 2070 rcp85 CanESM2 88 ## 6 2070 rcp85 CNRM-CM5 68 bkrfld_numhd_tbl %&gt;% group_by(year, scenario) %&gt;% summarize(avg_hd = mean(num_hotday), .groups = &quot;drop&quot;) %&gt;% tidyr::pivot_wider(id_cols = year, names_from = scenario, values_from = avg_hd) ## # A tibble: 30 x 3 ## year rcp45 rcp85 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2070 73.5 87.5 ## 2 2071 73.2 83 ## 3 2072 75 83.5 ## 4 2073 77.5 94.2 ## 5 2074 83.5 88.5 ## 6 2075 77.5 79 ## 7 2076 71 87 ## 8 2077 65.2 98.2 ## 9 2078 70.2 94.8 ## 10 2079 82 102 ## # ... with 20 more rows Sometimes you want to know the number of threshold events during a particular time of the year. For example tree crops are particularly susceptible to frost damage right after theyve bloomed. Lets compute the number of hot days in June, July and August, which can be particularly bad for nut development. Because we have daily data from 4 GCMs, we have to count the number of hot days for each GCM, and then average those together for each emissions scenario. bkrfld_sumrhd_tbl &lt;- bkrfld_tbl %&gt;% filter(cvar == &quot;tasmax&quot;, month(dt) %in% c(6,7,8)) %&gt;% mutate(hd = temp_f &gt;= units::set_units(38, degC), year = year(dt)) %&gt;% select(dt, year, cvar, scenario, gcm, temp_f, hd) bkrfld_sumrhd_tbl %&gt;% head() ## # A tibble: 6 x 7 ## dt year cvar scenario gcm temp_f hd ## &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; [degF] &lt;lgl&gt; ## 1 2070-06-01 2070 tasmax rcp45 HadGEM2-ES 79.8 FALSE ## 2 2070-06-02 2070 tasmax rcp45 HadGEM2-ES 77.7 FALSE ## 3 2070-06-03 2070 tasmax rcp45 HadGEM2-ES 74.7 FALSE ## 4 2070-06-04 2070 tasmax rcp45 HadGEM2-ES 80.6 FALSE ## 5 2070-06-05 2070 tasmax rcp45 HadGEM2-ES 86.3 FALSE ## 6 2070-06-06 2070 tasmax rcp45 HadGEM2-ES 92.9 FALSE bkrfld_numsumrhd_tbl &lt;- bkrfld_sumrhd_tbl %&gt;% group_by(year, scenario, gcm) %&gt;% summarise(num_sumrhd = sum(hd), .groups = &quot;drop&quot;) bkrfld_numsumrhd_tbl %&gt;% head() ## # A tibble: 6 x 4 ## year scenario gcm num_sumrhd ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2070 rcp45 CanESM2 64 ## 2 2070 rcp45 CNRM-CM5 43 ## 3 2070 rcp45 HadGEM2-ES 71 ## 4 2070 rcp45 MIROC5 55 ## 5 2070 rcp85 CanESM2 69 ## 6 2070 rcp85 CNRM-CM5 55 bkrfld_avgnumsumrhd_tbl &lt;- bkrfld_numsumrhd_tbl %&gt;% group_by(year, scenario) %&gt;% summarise(avg_num_sumrhd = mean(num_sumrhd), .groups = &quot;drop&quot;) bkrfld_avgnumsumrhd_tbl %&gt;% head() ## # A tibble: 6 x 3 ## year scenario avg_num_sumrhd ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2070 rcp45 58.2 ## 2 2070 rcp85 66.5 ## 3 2071 rcp45 56.8 ## 4 2071 rcp85 59 ## 5 2072 rcp45 60.5 ## 6 2072 rcp85 62.2 bkrfld_avgnumsumrhd_tbl %&gt;% tidyr::pivot_wider(id_cols = year, names_from = scenario, values_from = avg_num_sumrhd) ## # A tibble: 30 x 3 ## year rcp45 rcp85 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2070 58.2 66.5 ## 2 2071 56.8 59 ## 3 2072 60.5 62.2 ## 4 2073 65 72 ## 5 2074 63.5 69.2 ## 6 2075 59.8 55.2 ## 7 2076 56.5 70 ## 8 2077 54 70 ## 9 2078 59 72.5 ## 10 2079 59.8 74.2 ## # ... with 20 more rows 4.7 Counting Consecutive Events Heat spells, cold spells, and extreme precipitation events are all defined as consecutive days of a threshold event. The number of consecutive days may vary, but the general technique for identifying spells is Run an expression that tests whether the threshhold was passed for each day, returning a series TRUE or FALSE values. Pass the TRUE / FALSE values into the rle(), which identifies runs of TRUE and FALSE values Count the number of runs that meet the minimum duration To illustrate this, take the following series of 30 temperature values. Well compute the number of heat spells where the temperature was 100 or more for three or more days in a row: x_temp &lt;- c(96,97,101,98,100,102,101,99,94,89,97,102,104,101,103,99,92,94,88,90,98,101,99,103,104,102,98,97,98,99) Step 1 is to check to see if each value exceeds the threshold): x_hot &lt;- x_temp &gt;= 100 x_hot ## [1] FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE ## [13] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE ## [25] TRUE TRUE FALSE FALSE FALSE FALSE Next, feed the 30 TRUE/FALSE values into rle() (run-length encoding): rle_lst &lt;- rle(x_hot) rle_lst ## Run Length Encoding ## lengths: int [1:11] 2 1 1 3 4 4 6 1 1 3 ... ## values : logi [1:11] FALSE TRUE FALSE TRUE FALSE TRUE ... rle() returns a list with two elements. Both elements are vectors of the same length. The lengths element contains the number of contiguous identical elements found in a run in the original data. The values element contains the corresponding value of the run (in this case TRUE/FALSE values. Using this info, we can see our original data started with two FALSE values, followed by one TRUE value, followed by one FALSE value, followed by three TRUE values, and so on. To count the number of TRUE runs (aka spells) equal to or longer than n days, we can apply a simple expression: sum(rle_lst$values &amp; rle_lst$lengths &gt;= 3) ## [1] 3 Using these techniques, below we compute the number of heat spells where the temperature was 100 °F or more for three or more days in a row: bkrfld_sumrhd_tbl &lt;- bkrfld_tbl %&gt;% filter(cvar == &quot;tasmax&quot;, month(dt) %in% c(6,7,8)) %&gt;% mutate(hd = temp_f &gt;= units::set_units(38, degC), year = year(dt)) %&gt;% select(dt, year, cvar, scenario, gcm, temp_f, hd) bkrfld_sumrhd_tbl %&gt;% head() ## # A tibble: 6 x 7 ## dt year cvar scenario gcm temp_f hd ## &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; [degF] &lt;lgl&gt; ## 1 2070-06-01 2070 tasmax rcp45 HadGEM2-ES 79.8 FALSE ## 2 2070-06-02 2070 tasmax rcp45 HadGEM2-ES 77.7 FALSE ## 3 2070-06-03 2070 tasmax rcp45 HadGEM2-ES 74.7 FALSE ## 4 2070-06-04 2070 tasmax rcp45 HadGEM2-ES 80.6 FALSE ## 5 2070-06-05 2070 tasmax rcp45 HadGEM2-ES 86.3 FALSE ## 6 2070-06-06 2070 tasmax rcp45 HadGEM2-ES 92.9 FALSE We have to be a little creative to apply rle() in a data frame that has many time series in it (e.g., multiple years, GCMs, and emission scenarios). Each of the series needs to be fed into rle() individually, and the list returned by rle() will have different lengths. But what we can do is set up a list structure to store the results of rle(). First, we create a grouped tibble. A group tibble is still a tibble, but also has groups of rows invisibly defined. As we shall see shortly, other functions know what to do with those groups. bkrfld_grps_tbl &lt;- bkrfld_sumrhd_tbl %&gt;% group_by(year, scenario, gcm) %&gt;% arrange(dt) glimpse(bkrfld_grps_tbl) ## Rows: 22,080 ## Columns: 7 ## Groups: year, scenario, gcm [240] ## $ dt &lt;date&gt; 2070-06-01, 2070-06-01, 2070-06-01, 2070-06-01, 2070-06-01, ~ ## $ year &lt;dbl&gt; 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2~ ## $ cvar &lt;fct&gt; tasmax, tasmax, tasmax, tasmax, tasmax, tasmax, tasmax, tasma~ ## $ scenario &lt;fct&gt; rcp45, rcp45, rcp45, rcp45, rcp85, rcp85, rcp85, rcp85, rcp45~ ## $ gcm &lt;fct&gt; HadGEM2-ES, CNRM-CM5, CanESM2, MIROC5, HadGEM2-ES, CNRM-CM5, ~ ## $ temp_f [degF] 79.84528 [degF], 106.15573 [degF], 90.13398 [degF], 95.24855~ ## $ hd &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, ~ Next, we have to write a function that we can feed into group_modify(). If you read the documentation for group_modify(), it says the first two arguments of the function should accept i) a group of rows (as a tibble), ii) the properties of the group (e.g., which year, scenario, and gcm) as a 1-row tibble. our function should also return a tibble, that will be stacked for all the groups. In addition, group_modify() will also automatically include columns for the grouping variables. In our case, all we need is the number of heatspells so the function only has to return a 1x1 tibble. num_heatspells &lt;- function(data_tbl, key_tbl, num_days = 3) { ## Feed the hd column into rle() rle_lst &lt;- rle(data_tbl$hd) ## Return a tibble with one row tibble(num_spells = sum(rle_lst$values &amp; rle_lst$lengths &gt;= num_days)) } Now we can apply this function to every group: bkrfld_numspells_tbl &lt;- bkrfld_grps_tbl %&gt;% group_modify(.f = num_heatspells, num_days = 3) bkrfld_numspells_tbl %&gt;% head() ## # A tibble: 6 x 4 ## # Groups: year, scenario, gcm [6] ## year scenario gcm num_spells ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2070 rcp45 CanESM2 7 ## 2 2070 rcp45 CNRM-CM5 6 ## 3 2070 rcp45 HadGEM2-ES 5 ## 4 2070 rcp45 MIROC5 5 ## 5 2070 rcp85 CanESM2 3 ## 6 2070 rcp85 CNRM-CM5 6 Lastly, we can compute the average number of heatspells per emissions scenario: bkrfld_numspells_tbl %&gt;% group_by(scenario) %&gt;% summarise(avg_spells = mean(num_spells)) ## # A tibble: 2 x 2 ## scenario avg_spells ## &lt;fct&gt; &lt;dbl&gt; ## 1 rcp45 4.79 ## 2 rcp85 3.98 "],["05_agroclimate-metrics.html", "Chapter 5 AgroClimate Metrics 5.1 Load Packages 5.2 Fetch Some Data 5.3 Growing Degree Days 5.4 Chill Accumulation 5.5 Frost Days 5.6 Last Spring and First Fall Freeze 5.7 Freeze-Free Season 5.8 Tropical Nights and Hot Days 5.9 Extreme Heat Days 5.10 Heatwaves 5.11 Diurnal Temperature Range 5.12 Reference Evapotranspiration", " Chapter 5 AgroClimate Metrics Parker et al. (2022) identify a dozen agroclimate metrics derived 100% from weather data and provide physiologically relevant information for growers. This chapter provides R code to compute most of these agroclimate metrics using historic observed data from Cal-Adapt. 5.1 Load Packages As usual, we start by loading a bunch of packages into memory and specifying our preferences for conflicting function names: library(caladaptr) library(units) library(ggplot2) library(dplyr) library(tidyr) library(lubridate) library(sf) Set conflicted preferences: library(conflicted) conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;count&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;, quiet = TRUE) 5.2 Fetch Some Data To compute agroclimate metrics, well first get 20 years of observed data from the gridMet dataset for a location in Colusa County in the Sacramento Valley. colusa_cap &lt;- ca_loc_pt(coords = c(-122.159304, 39.289291)) %&gt;% ca_slug(c(&quot;tmmn_day_gridmet&quot;, &quot;tmmx_day_gridmet&quot;)) %&gt;% ca_years(start = 2000, end = 2020) colusa_cap ## Cal-Adapt API Request ## Location(s): ## x: -122.159 ## y: 39.289 ## Slug(s): tmmn_day_gridmet, tmmx_day_gridmet ## Dates: 2000-01-01 to 2020-12-31 ## colusa_cap %&gt;% ca_preflight() ## General issues ## - none found ## Issues for querying values ## - none found ## Issues for downloading rasters ## - none found plot(colusa_cap) Next we fetch the data. While were at it, well add columns for the climate variable (based on the slug), year, and temperature in Fahrenheit: colusa_tbl &lt;- colusa_cap %&gt;% ca_getvals_tbl() %&gt;% mutate(dt = as.Date(dt), year = year(as.Date(dt)), cvar = substr(slug, 1, 4), temp_f = units::set_units(val, degF)) %&gt;% select(year, dt, cvar, temp_f) glimpse(colusa_tbl) ## Rows: 15,342 ## Columns: 4 ## $ year &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200~ ## $ dt &lt;date&gt; 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01-05, 20~ ## $ cvar &lt;chr&gt; &quot;tmmn&quot;, &quot;tmmn&quot;, &quot;tmmn&quot;, &quot;tmmn&quot;, &quot;tmmn&quot;, &quot;tmmn&quot;, &quot;tmmn&quot;, &quot;tmmn&quot;,~ ## $ temp_f [degF] 32.45 [degF], 30.11 [degF], 32.09 [degF], 39.11 [degF], 32.27 ~ 5.3 Growing Degree Days Growing Degree Days (GDD) are a measure of accumulated heat starting from a specific date / event. You may wonder - whats the point of tracking accumulated heat, given that it cools down every night? The answer is because many plants seem to keep track of accumulated heat units. Research has shown that many phenological events, like the emergence of fruit, are strongly correlated with accumulated heat, also known as thermal time. Insect phenology is likewise strongly correlated with heat units. There are a few ways of computing GDD. Parker et al. (2022) recommend the simple average method with a base temperature of 10 °C: \\(GDD = (temp_{max} - temp_{min}) / 2 - temp_{base}\\) Negative GDD values are not biologically meaningful (i.e., plant development generally doesnt go backwards), so negative GDD values are generally converted to 0 (i.e., Method 1 described by McMaster and Wilhelm (1997)). Computing daily GDD is fairly straight-forward. We just have to remember to zero-out negative GDD values: (tbase_c &lt;- set_units(10, degC)) ## base temp ## 10 [°C] colusa_gdd_tbl &lt;- colusa_tbl %&gt;% mutate(temp_c = set_units(temp_f, degC)) %&gt;% ## create a degC column pivot_wider(id_cols = c(year, dt), ## make min and max temps separate colums names_from = cvar, values_from = temp_c) %&gt;% mutate(gdd = as.numeric(((tmmx+tmmn)/2) - tbase_c)) %&gt;% ## compute gdd as a numeric value mutate(gdd = if_else(gdd &lt; 0, 0, gdd)) ## zero-out negative values colusa_gdd_tbl %&gt;% head() ## # A tibble: 6 x 5 ## year dt tmmn tmmx gdd ## &lt;dbl&gt; &lt;date&gt; [°C] [°C] &lt;dbl&gt; ## 1 2000 2000-01-01 0.250 12.7 0 ## 2 2000 2000-01-02 -1.05 12.2 0 ## 3 2000 2000-01-03 0.0500 14.2 0 ## 4 2000 2000-01-04 3.95 9.65 0 ## 5 2000 2000-01-05 0.150 14.2 0 ## 6 2000 2000-01-06 -1.35 12.6 0 Applying GDD to predict crop development requires 1) a start date (also known as a biofix date), and 2) a crop phenology table. These are not shown here, but are not hard to apply (for an example see the Pistachio Nut Development Decision Support tool). 5.4 Chill Accumulation Chill accumulation is similar to growing degree days, but for cold. In other words, there are a few phenological events that appear to be strongly correlated with the accumulated amount of chill. An example of this is flowering for many tree crops. Apparently, the trees keep an internal ledger of how cold its been during the winter, and for how long. They use this internal record use to decide when its time to come out of their winter dormancy and start flowering. This mechanism probably evolved to help them avoid frost damage. Researchers have looked at a number of ways to measure accumulated chill, and the one that does the best job at predicting phenology events is called Chill Portions (CP) (Luedeling and Brown 2011). The calculations are a bit complicated, but fortunately theres a R-package that will compute chill portions. For more info, see here. 5.5 Frost Days Frost Days (FD) are the number of days per year with minimum temperatures (Tn)  0 °C (Parker et al. 2022). They can be computed with: colusa_fd_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmn&quot;, temp_f &lt;= set_units(0, degC)) %&gt;% group_by(year) %&gt;% summarise(fd = n()) colusa_fd_tbl ## # A tibble: 21 x 2 ## year fd ## &lt;dbl&gt; &lt;int&gt; ## 1 2000 9 ## 2 2001 14 ## 3 2002 16 ## 4 2003 10 ## 5 2004 9 ## 6 2005 5 ## 7 2006 11 ## 8 2007 23 ## 9 2008 8 ## 10 2009 24 ## # ... with 11 more rows 5.6 Last Spring and First Fall Freeze The Last Spring Freeze (LSF) is defined as the last day of the calendar year prior to 30 June with a Tn  0 °C. Conversely the First Fall Freeze (FFF) is defined as the first day of the calendar year commencing 1 July with Tn  0 °C (Parker et al. 2022). We can find the last freeze date by chaining together dplyr expressions that i) keep only freeze days from January through June, ii) group the freeze days by year, and iii) taking the max date for each group: colusa_lf_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmn&quot;, month(dt) &lt;= 6, temp_f &lt;= set_units(32, degF)) %&gt;% group_by(year) %&gt;% summarise(lf = max(dt)) colusa_lf_tbl ## # A tibble: 20 x 2 ## year lf ## &lt;dbl&gt; &lt;date&gt; ## 1 2000 2000-01-28 ## 2 2001 2001-04-10 ## 3 2002 2002-03-18 ## 4 2003 2003-02-08 ## 5 2004 2004-01-05 ## 6 2005 2005-01-30 ## 7 2006 2006-02-20 ## 8 2007 2007-03-01 ## 9 2008 2008-04-20 ## 10 2009 2009-03-23 ## 11 2011 2011-02-28 ## 12 2012 2012-03-23 ## 13 2013 2013-02-26 ## 14 2014 2014-02-04 ## 15 2015 2015-01-03 ## 16 2016 2016-01-01 ## 17 2017 2017-02-28 ## 18 2018 2018-03-05 ## 19 2019 2019-01-04 ## 20 2020 2020-01-04 Similarly, we can find the first fall freeze by keeping only dates from July - December where the temperature dipped below freezing, then taking the minimum date for each year: colusa_fff_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmn&quot;, month(dt) &gt;= 7, temp_f &lt;= set_units(32, degF)) %&gt;% group_by(year) %&gt;% summarise(fff = min(dt)) colusa_fff_tbl ## # A tibble: 20 x 2 ## year fff ## &lt;dbl&gt; &lt;date&gt; ## 1 2000 2000-11-12 ## 2 2001 2001-11-27 ## 3 2002 2002-12-23 ## 4 2003 2003-11-22 ## 5 2004 2004-11-29 ## 6 2005 2005-12-04 ## 7 2006 2006-11-28 ## 8 2007 2007-12-01 ## 9 2008 2008-12-10 ## 10 2009 2009-11-13 ## 11 2010 2010-11-23 ## 12 2011 2011-12-04 ## 13 2012 2012-12-15 ## 14 2013 2013-11-24 ## 15 2015 2015-11-25 ## 16 2016 2016-12-06 ## 17 2017 2017-12-11 ## 18 2018 2018-11-14 ## 19 2019 2019-11-24 ## 20 2020 2020-11-10 5.7 Freeze-Free Season The Freeze-Free Season (FFS) is calculated as the difference between the LSF and FFF (FFF [minus] LSF) (Parker et al. 2022). Since we already calculated LSF and FFF, computing the Freeze-Free Season can be done with a simple table join: colusa_lf_fff_tbl &lt;- colusa_lf_tbl %&gt;% left_join(colusa_fff_tbl, by = &quot;year&quot;) colusa_lf_fff_tbl %&gt;% head() ## # A tibble: 6 x 3 ## year lf fff ## &lt;dbl&gt; &lt;date&gt; &lt;date&gt; ## 1 2000 2000-01-28 2000-11-12 ## 2 2001 2001-04-10 2001-11-27 ## 3 2002 2002-03-18 2002-12-23 ## 4 2003 2003-02-08 2003-11-22 ## 5 2004 2004-01-05 2004-11-29 ## 6 2005 2005-01-30 2005-12-04 colusa_lf_fff_ffs_tbl &lt;- colusa_lf_fff_tbl %&gt;% mutate(ffs = fff - lf) colusa_lf_fff_ffs_tbl ## # A tibble: 20 x 4 ## year lf fff ffs ## &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;drtn&gt; ## 1 2000 2000-01-28 2000-11-12 289 days ## 2 2001 2001-04-10 2001-11-27 231 days ## 3 2002 2002-03-18 2002-12-23 280 days ## 4 2003 2003-02-08 2003-11-22 287 days ## 5 2004 2004-01-05 2004-11-29 329 days ## 6 2005 2005-01-30 2005-12-04 308 days ## 7 2006 2006-02-20 2006-11-28 281 days ## 8 2007 2007-03-01 2007-12-01 275 days ## 9 2008 2008-04-20 2008-12-10 234 days ## 10 2009 2009-03-23 2009-11-13 235 days ## 11 2011 2011-02-28 2011-12-04 279 days ## 12 2012 2012-03-23 2012-12-15 267 days ## 13 2013 2013-02-26 2013-11-24 271 days ## 14 2014 2014-02-04 NA NA days ## 15 2015 2015-01-03 2015-11-25 326 days ## 16 2016 2016-01-01 2016-12-06 340 days ## 17 2017 2017-02-28 2017-12-11 286 days ## 18 2018 2018-03-05 2018-11-14 254 days ## 19 2019 2019-01-04 2019-11-24 324 days ## 20 2020 2020-01-04 2020-11-10 311 days 5.8 Tropical Nights and Hot Days Tropical Nights (TRN) are calculated as the number of nights per year with Tn &gt; 20 °C (68 °F) (Parker et al. 2022). This can be computed with: colusa_tn_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmn&quot;, temp_f &gt; set_units(20, degC)) %&gt;% group_by(year) %&gt;% summarise(tn = n()) colusa_tn_tbl ## # A tibble: 18 x 2 ## year tn ## &lt;dbl&gt; &lt;int&gt; ## 1 2000 10 ## 2 2001 8 ## 3 2002 3 ## 4 2003 10 ## 5 2004 1 ## 6 2005 4 ## 7 2006 12 ## 8 2007 6 ## 9 2008 6 ## 10 2009 3 ## 11 2011 2 ## 12 2012 3 ## 13 2013 13 ## 14 2015 10 ## 15 2017 15 ## 16 2018 2 ## 17 2019 3 ## 18 2020 13 Hot Days (HD) are defined as when Tx &gt; 38 °C (Parker et al. 2022). The number of hot days per year can be computed with: colusa_hd_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmx&quot;, temp_f &gt; set_units(38, degC)) %&gt;% group_by(year) %&gt;% summarise(hd = n()) colusa_hd_tbl ## # A tibble: 21 x 2 ## year hd ## &lt;dbl&gt; &lt;int&gt; ## 1 2000 15 ## 2 2001 17 ## 3 2002 15 ## 4 2003 14 ## 5 2004 7 ## 6 2005 20 ## 7 2006 20 ## 8 2007 8 ## 9 2008 21 ## 10 2009 26 ## # ... with 11 more rows 5.9 Extreme Heat Days Extreme Heat Days (EHD) are the number of days per year with Tx &gt;98th percentile of summer (June-August) Tx for the 19812010 period (Parker et al. 2022). This is similar to HD, but with a threshold value based on the historic record. We can compute the 98th percentile of daily summertime highs with: colusa_ehd_thresh &lt;- ca_loc_pt(coords = c(-122.159304, 39.289291)) %&gt;% ca_slug(&quot;tmmx_day_gridmet&quot;) %&gt;% ca_years(start = 1981, end = 2010) %&gt;% ca_getvals_tbl(quiet = TRUE) %&gt;% filter(month(as.Date(dt)) %in% c(6,7,8)) %&gt;% pull(val) %&gt;% quantile(0.98) %&gt;% set_units(degF) colusa_ehd_thresh ## 106.2176 [degF] Once we have that threshold, we can compute Extreme Heat Days with: colusa_ehd_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmx&quot;, temp_f &gt; colusa_ehd_thresh) %&gt;% group_by(year) %&gt;% summarise(hd = n()) colusa_ehd_tbl ## # A tibble: 17 x 2 ## year hd ## &lt;dbl&gt; &lt;int&gt; ## 1 2001 1 ## 2 2002 1 ## 3 2003 4 ## 4 2005 2 ## 5 2006 7 ## 6 2007 1 ## 7 2008 4 ## 8 2009 3 ## 9 2010 1 ## 10 2012 10 ## 11 2013 12 ## 12 2014 2 ## 13 2015 1 ## 14 2017 8 ## 15 2018 1 ## 16 2019 1 ## 17 2020 3 5.10 Heatwaves Heatwave events (HW) are defined as 3+ consecutive days with Tx &gt; 98th percentile of 19812010 summer Tx (as in EHD) (Parker et al. 2022). Using the technique described in Chapter 4 - Counting Consecutive Events, we can compute the number of heatwaves per year. Add a column for extreme heat day, then create a grouped tibble (by year): colusa_grpd_tbl &lt;- colusa_tbl %&gt;% filter(cvar == &quot;tmmx&quot;) %&gt;% mutate(ehd = temp_f &gt; colusa_ehd_thresh) %&gt;% group_by(year) %&gt;% arrange(dt) glimpse(colusa_grpd_tbl) ## Rows: 7,671 ## Columns: 5 ## Groups: year [21] ## $ year &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200~ ## $ dt &lt;date&gt; 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01-05, 20~ ## $ cvar &lt;chr&gt; &quot;tmmx&quot;, &quot;tmmx&quot;, &quot;tmmx&quot;, &quot;tmmx&quot;, &quot;tmmx&quot;, &quot;tmmx&quot;, &quot;tmmx&quot;, &quot;tmmx&quot;,~ ## $ temp_f [degF] 54.95 [degF], 53.87 [degF], 57.65 [degF], 49.37 [degF], 57.47 ~ ## $ ehd &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ~ Create a function that we can pass to group_modify(), will return the number of heatwaves per group (year): num_hw &lt;- function(data_tbl, key_tbl, num_days = 3) { rle_lst &lt;- rle(data_tbl$ehd) tibble(num_hw = sum(rle_lst$values &amp; rle_lst$lengths &gt;= num_days)) }   Apply the heatwave function to the grouped tibble: colusa_hw_tbl &lt;- colusa_grpd_tbl %&gt;% group_modify(.f = num_hw, num_days = 3) colusa_hw_tbl ## # A tibble: 21 x 2 ## # Groups: year [21] ## year num_hw ## &lt;dbl&gt; &lt;int&gt; ## 1 2000 0 ## 2 2001 0 ## 3 2002 0 ## 4 2003 0 ## 5 2004 0 ## 6 2005 0 ## 7 2006 1 ## 8 2007 0 ## 9 2008 1 ## 10 2009 0 ## # ... with 11 more rows 5.11 Diurnal Temperature Range Diurnal Temperature Range (DTR) is the difference between daily Tx and Tn (Parker et al. 2022). Below we calculate DTR over 1 March to 1 November. colusa_dtr_tbl &lt;- colusa_tbl %&gt;% filter(month(dt) %in% 3:10) %&gt;% pivot_wider(id_cols = c(year, dt), names_from = cvar, values_from = temp_f) %&gt;% mutate(dtr = tmmx - tmmn) head(colusa_dtr_tbl) ## # A tibble: 6 x 5 ## year dt tmmn tmmx dtr ## &lt;dbl&gt; &lt;date&gt; [degF] [degF] [degF] ## 1 2000 2000-03-01 35.7 62.7 27.0 ## 2 2000 2000-03-02 45.9 55.5 9.54 ## 3 2000 2000-03-03 36.6 68.6 32.0 ## 4 2000 2000-03-04 41.1 62.1 21.1 ## 5 2000 2000-03-05 46.3 55.5 9.18 ## 6 2000 2000-03-06 43.6 56.4 12.8 ggplot(colusa_dtr_tbl %&gt;% mutate(year = as.factor(year)), aes(x=year, y = dtr)) + geom_boxplot() + labs(title = &quot;Diurnal Temperature Range, 2000-2020&quot;, subtitle = &quot;Colusa, CA&quot;, caption = &quot;Dataset: gridMet. Temporal period: March - October&quot;, x = &quot;&quot;, y = &quot;daily temperature range&quot;) 5.12 Reference Evapotranspiration ETo is calculated following the FAO PenmanMonteith method (Allen and United Nations 1998). We calculate summer (June-August) average ETo for each year 19812020 for our analysis. ETo units are mm (Parker et al. 2022). More coming soon "],["06_degday.html", "Chapter 6 Degree Days", " Chapter 6 Degree Days A common application of weather and climate data is predicting phenological events like egg laying for insects, or fruit ripening for tree crops. These predictions are possible due to research which has modeled the relationship between phenology and accumulated heat, or degree days. The researchers who study the relationship between phenology and accumulated heat use several calculations to measure degree days. Many of the most common degree day formulas can be found in the degday R package. https://ucanr-igis.github.io/degday/ All you need after that are the phenology tables. Example: https://ucanr-igis.shinyapps.io/noworm/ More coming soon "],["07_joining-other-data.html", "Chapter 7 Joining Other Tables", " Chapter 7 Joining Other Tables Most analyses of climate impacts combine Coming soon "],["08_raster-analyses.html", "Chapter 8 Raster Operations 8.1 Load Packages 8.2 Fetch TIFs 8.3 Import TIFs in R", " Chapter 8 Raster Operations Downloading Cal-Adapt climate data as rasters makes sense if your analysis involves methods that require the entire surface, and/or uses raster methods such as delineating isopleths, classification, or stacking on other raster layers. You may also want to use rasters simply for convenience, for example if you have a lot of sample points and/or will be extracting values many times. Downloading rasters only has to be done once, after which you can query them locally. In this chapter, well see how to download rasters to compute the number of frost days across Fresno county, which is a large county that includes both valley and mountainous areas. Our first output will be a raster that shows the number of frost days per year for 30 years of observed minimum daily temperature. 8.1 Load Packages As usual, we start by loading a bunch of packages into memory and specifying our preferences for conflicting function names: library(caladaptr) library(units) library(ggplot2) library(dplyr) library(tidyr) library(lubridate) library(sf) library(stars) Set conflicted preferences: library(conflicted) conflict_prefer(&quot;filter&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;count&quot;, &quot;dplyr&quot;, quiet = TRUE) conflict_prefer(&quot;select&quot;, &quot;dplyr&quot;, quiet = TRUE) 8.2 Fetch TIFs Well start by computing the number of frost days from observed historic temperature data from the Livneh dataset. fres_cap &lt;- ca_loc_aoipreset(type = &quot;counties&quot;, idfld = &quot;fips&quot;, idval = &quot;06019&quot;) %&gt;% ca_livneh(TRUE) %&gt;% ca_cvar(&quot;tasmin&quot;) %&gt;% ca_period(&quot;day&quot;) %&gt;% ca_years(start = 1981, end = 2010) fres_cap %&gt;% ca_preflight(check_for = &quot;getrst&quot;) ## General issues ## - none found ## Issues for downloading rasters ## - none found plot(fres_cap, locagrid = TRUE) To download TIF files, we need to specify an output directory. Below well put them in a directory under Rs home directory (i.e., My Documents): (tiffs_dir &lt;- file.path(&quot;~&quot;, &quot;ca_data/fresno&quot;)) ## [1] &quot;~/ca_data/fresno&quot; if (!file.exists(tiffs_dir)) dir.create(tiffs_dir, recursive = TRUE) Now we can download the tif(s) using ca_getrst_stars(): fres_fn &lt;- fres_cap %&gt;% ca_getrst_stars(out_dir = tiffs_dir, mask = TRUE, quiet = FALSE, normalize_path = TRUE, overwrite = FALSE) ## - ~/ca_data/fresno/tasmin_day_livneh_fips-06019.tif found. Skipping. ## - Done. Read TIFs in with `ca_stars_read` list.files(tiffs_dir) ## [1] &quot;tasmin_day_livneh_fips-06019.attr.rds&quot; ## [2] &quot;tasmin_day_livneh_fips-06019.tif&quot; 8.3 Import TIFs in R At this point we have TIF files, but they are not loaded into R. The recommended way to import them is as stars objects using: fres_star_lst &lt;- fres_fn %&gt;% ca_stars_read() ca_stars_read() returns a list. Lets see whats in it: class(fres_star_lst) ## [1] &quot;list&quot; length(fres_star_lst) ## [1] 1 class(fres_star_lst[[1]]) ## [1] &quot;stars&quot; fres_star_lst[[1]] ## stars object with 3 dimensions and 1 attribute ## attribute(s), summary of first 1e+05 cells: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## tasmin_day_livneh_fips-06019 -22.16 -4.13 2.95 0.7203694 6.13 13.61 ## NA&#39;s ## tasmin_day_livneh_fips-06019 65915 ## dimension(s): ## from to offset delta refsys point values x/y ## x 1 42 -120.938 0.0625 WGS 84 FALSE NULL [x] ## y 1 28 37.625 -0.0625 WGS 84 FALSE NULL [y] ## date 1 10957 1981-01-01 1 days Date NA NULL In summary, our stars object has 42 columns and 28 rows, and 10,957 bands or layers, which is one layer for each day (365 * 30 = 10,950). Next, we have compute the number of frost days per year. Normally, we would try to split the date dimension into two (year and Julian date), but that wont work because years have different number of days. Instead well loop through the years, generate a stars object with the number of freeze days, and then combine them all back together. Step 1 is to generate the years: ## Get the years from the values of the &quot;date&quot; dimension years_int &lt;- fres_star_lst[[1]] %&gt;% st_get_dimension_values(&quot;date&quot;) %&gt;% as.Date() %&gt;% year() %&gt;% unique() %&gt;% sort() years_int ## [1] 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 ## [16] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Next, create an empty list to hold stars objects: numfrzday_lst &lt;- list() Next, loop through the years and add a stars object to the list: for (i in 1:length(years_int)) { yr &lt;- years_int[i] numfrzday_lst[[as.character(yr)]] &lt;- (fres_star_lst[[1]] %&gt;% filter(year(date) == yr) &lt; 0) %&gt;% setNames(&quot;freeze_yn&quot;) %&gt;% st_apply(c(&quot;x&quot;, &quot;y&quot;), sum) %&gt;% setNames(&quot;num_fd&quot;) } names(numfrzday_lst) ## [1] &quot;1981&quot; &quot;1982&quot; &quot;1983&quot; &quot;1984&quot; &quot;1985&quot; &quot;1986&quot; &quot;1987&quot; &quot;1988&quot; &quot;1989&quot; &quot;1990&quot; ## [11] &quot;1991&quot; &quot;1992&quot; &quot;1993&quot; &quot;1994&quot; &quot;1995&quot; &quot;1996&quot; &quot;1997&quot; &quot;1998&quot; &quot;1999&quot; &quot;2000&quot; ## [21] &quot;2001&quot; &quot;2002&quot; &quot;2003&quot; &quot;2004&quot; &quot;2005&quot; &quot;2006&quot; &quot;2007&quot; &quot;2008&quot; &quot;2009&quot; &quot;2010&quot; Combine the elements of numfrzday_lst into a single stars object: fres_numfd_year &lt;- do.call(c, c(numfrzday_lst, list(along = list(year = years_int)))) fres_numfd_year ## stars object with 3 dimensions and 1 attribute ## attribute(s): ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## num_fd 0 12 30 88.15885 173 351 23250 ## dimension(s): ## from to offset delta refsys point values x/y ## x 1 42 -120.938 0.0625 WGS 84 FALSE NULL [x] ## y 1 28 37.625 -0.0625 WGS 84 FALSE NULL [y] ## year 1 30 1981 1 NA NA NULL Now we can plot: plot(fres_numfd_year, axes = TRUE) More coming soon "],["09_shiny.html", "Chapter 9 Shiny Apps", " Chapter 9 Shiny Apps caladaptr can be used in Shiny Apps to retrieve data from the API on-the-fly. To try out some of these apps, see https://github.com/ucanr-igis/caladaptr.apps To show a spinner while data is being fetched, you can do this exampe: show_spinner() my_vals &lt;- ca_getvals_tbl(pt_cap(), quiet = TRUE) hide_spinner() To show a progress bar that updates as data is being retrieved, you can pass a Shiny progress object to ca_getvals_tbl() (example): progress &lt;- shiny::Progress$new() on.exit(progress$close()) progress$set(value = 0, message = &quot;Fetching data from Cal-Adapt:&quot;) ## Get values pt_daytmp_tbl &lt;- ca_getvals_tbl(pt_prj_cap(), quiet = TRUE, shiny_progress = progress) Additional Tips: Deploying your Shiny app without being blocked If youll be hosting your ShinyApp on the cloud, theres a slight possibility that the IP address might be blocked by Cal-Adapt (due to precautions theyve had to take to defend against DOS attacks) ShinyApps.io works fine (those IP addresses have been white listed) GoogleCloud IP addresses are all blocked If this is a problem, one solution is to 1) get a VM with a stable IP address, 2) ask the administrator of the Cal-Adapt API to white list that IP address. Memory Also, be cognizant of the amount of RAM memory you have to work with. Climate data can quickly consume hundreds of megabytes, and its not uncommon for R to make copies of data frames when doing manipulations. If youre working on a VM that only has 1 GB of RAM, you can quickly run out of memory. This can be difficult to diagnose, but if your app is working fine on your laptop, but disconnected unexpectedly when you run it on ShinyApps.io, then check the logs for an out-of-memory error. Your options for dealing with out-of-memory errors include getting a VM with more RAM (which could cost you $), using ca_getvals_db() (which dumps values into a SQLite database) instead of ca_getvals_tbl(), redesigning your app for more efficient use of memory, or making your app available as a zip file that users can run locally from their RStudio. More coming soon "],["99_references.html", "Chapter 10 References", " Chapter 10 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
